{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap07_Convolutional_neural_network(CNN).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPXdawOW3GOK23fQ+ApiHzu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXFHY_WipRfC",
        "outputId": "bb40b338-d8ed-4d7b-9a9b-e9b2cf53e69e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "iRnJt2n8pTue"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4차원 배열**"
      ],
      "metadata": {
        "id": "yCnhv64Dpcck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.random.rand(10, 1, 28, 28) # 높이 28, 너비 28, 채널 1개인 데이터가 10개\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G951Fxlsps-l",
        "outputId": "28024d14-4c8b-438f-b68f-22dadd1c1321"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0].shape # 첫 번째 데이터에 접근"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPF4Z6dtp2Ez",
        "outputId": "8dac1985-5cef-495e-bfe6-9ea46371d877"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0][0] # 첫 번째 데이터의 첫 채널의 공간 데이터에 접근"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5ZdONt8p5Gt",
        "outputId": "5ac41eab-a052-49c0-9f1e-91c4b9dbbead"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.39847256, 0.1997792 , 0.35920803, 0.6175381 , 0.00271316,\n",
              "        0.83845123, 0.23060389, 0.59667238, 0.19776184, 0.78727703,\n",
              "        0.50564659, 0.78703307, 0.02558828, 0.11321511, 0.38514646,\n",
              "        0.76462539, 0.11741679, 0.34388019, 0.31990199, 0.80048197,\n",
              "        0.33600489, 0.03487236, 0.89753161, 0.96007563, 0.0690366 ,\n",
              "        0.48609242, 0.99642828, 0.16543936],\n",
              "       [0.27496271, 0.95216997, 0.51456434, 0.2569952 , 0.90169588,\n",
              "        0.96425148, 0.31768271, 0.21508249, 0.39516769, 0.40708922,\n",
              "        0.86198259, 0.18615325, 0.00676358, 0.24712187, 0.59529668,\n",
              "        0.5627158 , 0.1360914 , 0.33692011, 0.14953099, 0.72315748,\n",
              "        0.94405169, 0.30823754, 0.20629883, 0.34087127, 0.03114606,\n",
              "        0.98852759, 0.68315548, 0.13089124],\n",
              "       [0.44502511, 0.62677531, 0.98451539, 0.40498763, 0.84739428,\n",
              "        0.3113402 , 0.75163855, 0.80391891, 0.26550603, 0.80225167,\n",
              "        0.58531889, 0.48079878, 0.88562754, 0.99486731, 0.9076289 ,\n",
              "        0.95203011, 0.17622466, 0.01989232, 0.05284403, 0.92611067,\n",
              "        0.0901621 , 0.33517351, 0.04372711, 0.79844146, 0.58639602,\n",
              "        0.65351555, 0.90558925, 0.99174256],\n",
              "       [0.43235679, 0.68300447, 0.63340188, 0.31573065, 0.3709397 ,\n",
              "        0.59699987, 0.20727756, 0.74072899, 0.24293484, 0.36004544,\n",
              "        0.50199454, 0.75516137, 0.75162458, 0.68864687, 0.77561105,\n",
              "        0.12342453, 0.90826069, 0.04782624, 0.51427904, 0.37119809,\n",
              "        0.187618  , 0.80737308, 0.88658286, 0.47154448, 0.94182595,\n",
              "        0.82982869, 0.70995353, 0.89787294],\n",
              "       [0.41508522, 0.67067508, 0.61097509, 0.60356958, 0.09197656,\n",
              "        0.83472073, 0.45706952, 0.45938863, 0.68249102, 0.3832623 ,\n",
              "        0.08232693, 0.55531076, 0.04042539, 0.59257711, 0.5933489 ,\n",
              "        0.01055716, 0.70859776, 0.76864918, 0.34411854, 0.33275528,\n",
              "        0.01634505, 0.48032755, 0.57749991, 0.3863387 , 0.23902327,\n",
              "        0.70547474, 0.63353759, 0.04838298],\n",
              "       [0.13288333, 0.8073196 , 0.74746099, 0.33361638, 0.13891515,\n",
              "        0.44294715, 0.74802945, 0.93519043, 0.22725177, 0.98423885,\n",
              "        0.32769805, 0.98727261, 0.61009836, 0.78640071, 0.66652751,\n",
              "        0.97730817, 0.68328253, 0.25080367, 0.43692216, 0.16476891,\n",
              "        0.38335425, 0.46511601, 0.70149589, 0.13929712, 0.14198674,\n",
              "        0.97188856, 0.41002824, 0.67495405],\n",
              "       [0.78465562, 0.47084372, 0.67795942, 0.16771699, 0.44004698,\n",
              "        0.74198096, 0.98028801, 0.29994176, 0.17335899, 0.34378976,\n",
              "        0.13178072, 0.72084304, 0.5581017 , 0.00993965, 0.97831382,\n",
              "        0.79522723, 0.41181132, 0.24100195, 0.44266999, 0.84974098,\n",
              "        0.50501725, 0.50006497, 0.59436209, 0.3983331 , 0.59472083,\n",
              "        0.55875477, 0.72244056, 0.63471915],\n",
              "       [0.88754958, 0.55390728, 0.97069076, 0.53589447, 0.09015834,\n",
              "        0.64624656, 0.26943323, 0.01540083, 0.57453402, 0.33537867,\n",
              "        0.67403093, 0.80716698, 0.06160467, 0.01212655, 0.85783642,\n",
              "        0.28377203, 0.58248081, 0.09470419, 0.97480033, 0.13063359,\n",
              "        0.52944618, 0.40098525, 0.05228008, 0.05692116, 0.5027348 ,\n",
              "        0.63564405, 0.23462314, 0.99213811],\n",
              "       [0.78275391, 0.33902228, 0.15151899, 0.98602162, 0.31393661,\n",
              "        0.07691197, 0.53523679, 0.58510075, 0.83018693, 0.17594141,\n",
              "        0.38465383, 0.54126772, 0.65720399, 0.97421216, 0.23909646,\n",
              "        0.44904921, 0.00553558, 0.13255035, 0.46554671, 0.70449084,\n",
              "        0.41169494, 0.29483775, 0.25441322, 0.67506989, 0.33952165,\n",
              "        0.2786421 , 0.45159271, 0.13431496],\n",
              "       [0.83500807, 0.91996643, 0.638476  , 0.60404234, 0.18369109,\n",
              "        0.48542216, 0.62784014, 0.32402367, 0.46536292, 0.72513051,\n",
              "        0.98107205, 0.83536494, 0.15411888, 0.22942604, 0.18548844,\n",
              "        0.50921323, 0.53390136, 0.97391724, 0.27967835, 0.93820049,\n",
              "        0.95211335, 0.25868781, 0.10866734, 0.25536568, 0.89634665,\n",
              "        0.86471231, 0.06032054, 0.18581164],\n",
              "       [0.18440682, 0.71523206, 0.77385103, 0.33587842, 0.30444201,\n",
              "        0.02680356, 0.12807009, 0.86722205, 0.45125603, 0.57307881,\n",
              "        0.67016042, 0.7680216 , 0.98061524, 0.4979786 , 0.31551312,\n",
              "        0.42669235, 0.03891433, 0.01340962, 0.50286589, 0.59233483,\n",
              "        0.98532551, 0.38146085, 0.63094461, 0.66051378, 0.51940293,\n",
              "        0.62253104, 0.52327218, 0.85460926],\n",
              "       [0.67595633, 0.20920057, 0.97217234, 0.66161006, 0.54396561,\n",
              "        0.55655755, 0.46822647, 0.64610879, 0.203376  , 0.90141973,\n",
              "        0.29744375, 0.88335451, 0.66116501, 0.16823075, 0.32967829,\n",
              "        0.49797404, 0.12021475, 0.34405455, 0.70630257, 0.08667434,\n",
              "        0.63417895, 0.27478543, 0.18085034, 0.87395853, 0.52621691,\n",
              "        0.4435743 , 0.81554459, 0.99291544],\n",
              "       [0.85941364, 0.91269319, 0.13026007, 0.92697567, 0.57802041,\n",
              "        0.87697427, 0.59086603, 0.86918443, 0.85950227, 0.69871954,\n",
              "        0.51744179, 0.688879  , 0.18365804, 0.88155647, 0.60376344,\n",
              "        0.70783497, 0.23455513, 0.43380829, 0.28922579, 0.89377322,\n",
              "        0.10173041, 0.62781161, 0.46400951, 0.99422552, 0.95979735,\n",
              "        0.25462607, 0.13285742, 0.33784753],\n",
              "       [0.14217649, 0.3654179 , 0.96958364, 0.6837485 , 0.80087137,\n",
              "        0.9249354 , 0.95387224, 0.18760409, 0.66436453, 0.18417385,\n",
              "        0.90825878, 0.57002104, 0.4605034 , 0.15241786, 0.19933623,\n",
              "        0.90692055, 0.61074719, 0.77438549, 0.15109397, 0.053137  ,\n",
              "        0.60511086, 0.5552674 , 0.53131008, 0.29636826, 0.53359035,\n",
              "        0.72749048, 0.56958223, 0.51618617],\n",
              "       [0.338434  , 0.17632268, 0.16779382, 0.20045722, 0.43780006,\n",
              "        0.6703935 , 0.03273829, 0.24567704, 0.5562729 , 0.52064271,\n",
              "        0.99819818, 0.70298071, 0.4376158 , 0.4234724 , 0.96798962,\n",
              "        0.27965829, 0.02173363, 0.76822883, 0.94827683, 0.68294763,\n",
              "        0.81861863, 0.0511268 , 0.59252942, 0.36021   , 0.84494675,\n",
              "        0.7446473 , 0.37066909, 0.63909356],\n",
              "       [0.00881402, 0.81603999, 0.4918713 , 0.36140569, 0.13270826,\n",
              "        0.25039909, 0.95810054, 0.52425906, 0.98264666, 0.25304911,\n",
              "        0.75778265, 0.91396639, 0.38601036, 0.10744746, 0.23971534,\n",
              "        0.95945092, 0.78879316, 0.97033861, 0.56204913, 0.8305342 ,\n",
              "        0.89069679, 0.57611419, 0.28713892, 0.22915675, 0.93023122,\n",
              "        0.97795379, 0.89959291, 0.53014788],\n",
              "       [0.85188047, 0.61422323, 0.54043994, 0.36046599, 0.75398075,\n",
              "        0.76661725, 0.55720672, 0.29895933, 0.72567292, 0.08412331,\n",
              "        0.22626756, 0.41403209, 0.50994503, 0.74911336, 0.50642463,\n",
              "        0.79064574, 0.57689087, 0.26920095, 0.55244492, 0.68969947,\n",
              "        0.78445381, 0.28059021, 0.82603345, 0.54800297, 0.02792895,\n",
              "        0.98131251, 0.28023521, 0.82383609],\n",
              "       [0.66522481, 0.77780296, 0.99058104, 0.73169929, 0.36608843,\n",
              "        0.09874696, 0.71594448, 0.34966436, 0.42758844, 0.22976375,\n",
              "        0.26990018, 0.36435113, 0.92808241, 0.53664159, 0.22176205,\n",
              "        0.29195291, 0.93472361, 0.2748639 , 0.27156703, 0.95226386,\n",
              "        0.91697715, 0.10623897, 0.07943813, 0.85974241, 0.34641332,\n",
              "        0.44496131, 0.77948981, 0.56122341],\n",
              "       [0.27784872, 0.49872166, 0.01975404, 0.73615909, 0.48625673,\n",
              "        0.81232167, 0.24142505, 0.9496695 , 0.72475953, 0.4610056 ,\n",
              "        0.92198705, 0.9904792 , 0.48956532, 0.35089108, 0.85951809,\n",
              "        0.97030997, 0.46150143, 0.09862895, 0.4974137 , 0.70782791,\n",
              "        0.57452546, 0.62388909, 0.02672865, 0.83741533, 0.48506594,\n",
              "        0.13266555, 0.62693066, 0.55947052],\n",
              "       [0.28394567, 0.45536592, 0.67274261, 0.6434421 , 0.628475  ,\n",
              "        0.80425697, 0.41165986, 0.34773165, 0.59896888, 0.43429717,\n",
              "        0.68670093, 0.86288203, 0.19808466, 0.12490049, 0.91870706,\n",
              "        0.50980631, 0.44557915, 0.04767464, 0.90465687, 0.37422932,\n",
              "        0.53925072, 0.24769796, 0.01684364, 0.99104273, 0.80449294,\n",
              "        0.22319248, 0.27113846, 0.86336274],\n",
              "       [0.95641972, 0.49326368, 0.22823291, 0.53185513, 0.33923685,\n",
              "        0.09768915, 0.38675787, 0.11039009, 0.13726535, 0.55704299,\n",
              "        0.75688135, 0.31571928, 0.03214096, 0.36111281, 0.67854623,\n",
              "        0.62230123, 0.18339102, 0.31813952, 0.36202007, 0.10409736,\n",
              "        0.47709441, 0.90519337, 0.07451861, 0.34001663, 0.1655074 ,\n",
              "        0.55912906, 0.9320812 , 0.80404965],\n",
              "       [0.98929544, 0.01223602, 0.2195915 , 0.33886103, 0.76987852,\n",
              "        0.58384493, 0.68778407, 0.08536893, 0.59993987, 0.56669543,\n",
              "        0.84558199, 0.45712393, 0.6649973 , 0.12577074, 0.02902595,\n",
              "        0.23654368, 0.88770546, 0.18818697, 0.90619832, 0.65609311,\n",
              "        0.45548625, 0.01101021, 0.93286237, 0.57452325, 0.69707931,\n",
              "        0.20932274, 0.09000164, 0.55032069],\n",
              "       [0.13834598, 0.25960117, 0.75186312, 0.67593561, 0.60913338,\n",
              "        0.77320733, 0.23704378, 0.08225624, 0.46230605, 0.24658334,\n",
              "        0.65738227, 0.13570707, 0.09745517, 0.71922809, 0.84621756,\n",
              "        0.47968496, 0.67921273, 0.36472766, 0.29793267, 0.4761494 ,\n",
              "        0.02903665, 0.56777029, 0.18849296, 0.01062881, 0.5565716 ,\n",
              "        0.91217156, 0.94114023, 0.48882828],\n",
              "       [0.06537433, 0.73973279, 0.01095565, 0.31348772, 0.77711918,\n",
              "        0.94777708, 0.92376262, 0.59586554, 0.2200501 , 0.9964642 ,\n",
              "        0.4602111 , 0.61076496, 0.54603415, 0.79854509, 0.97141678,\n",
              "        0.77604408, 0.70902169, 0.87757416, 0.96744375, 0.10262535,\n",
              "        0.58419523, 0.32758417, 0.97523422, 0.11191395, 0.65264438,\n",
              "        0.61069365, 0.18904328, 0.53462924],\n",
              "       [0.99993647, 0.58234862, 0.45879361, 0.36271481, 0.26203709,\n",
              "        0.05628878, 0.67704582, 0.88085542, 0.91519264, 0.43189549,\n",
              "        0.94590272, 0.8152852 , 0.02215574, 0.89393295, 0.34279926,\n",
              "        0.23326026, 0.03733163, 0.3804617 , 0.97354791, 0.98851135,\n",
              "        0.24227857, 0.40395642, 0.62715417, 0.60517435, 0.973202  ,\n",
              "        0.45250703, 0.11461467, 0.09947829],\n",
              "       [0.28849118, 0.05774465, 0.38197172, 0.15275633, 0.0768403 ,\n",
              "        0.55149134, 0.57057719, 0.4441681 , 0.57337375, 0.116436  ,\n",
              "        0.53873548, 0.73678196, 0.68295087, 0.75601628, 0.78399116,\n",
              "        0.96515629, 0.25220209, 0.19511795, 0.01779001, 0.20991861,\n",
              "        0.94843043, 0.02844983, 0.45375521, 0.16262956, 0.02265012,\n",
              "        0.58379832, 0.56840864, 0.61550214],\n",
              "       [0.86911443, 0.53427864, 0.47749702, 0.64889999, 0.65932452,\n",
              "        0.64973228, 0.71025023, 0.24026984, 0.0509787 , 0.1311225 ,\n",
              "        0.0324596 , 0.04800148, 0.36514862, 0.93442373, 0.10957709,\n",
              "        0.59678998, 0.58095955, 0.72039696, 0.6025447 , 0.02495175,\n",
              "        0.69743467, 0.09889144, 0.51497745, 0.91775884, 0.58989639,\n",
              "        0.34675825, 0.65352416, 0.78574222],\n",
              "       [0.44933224, 0.51754095, 0.23215507, 0.58141605, 0.91878976,\n",
              "        0.16073353, 0.08669689, 0.94525913, 0.72386268, 0.44594125,\n",
              "        0.39291784, 0.24627097, 0.79338941, 0.12192793, 0.10578299,\n",
              "        0.8161472 , 0.5700702 , 0.65109148, 0.07746079, 0.89061502,\n",
              "        0.33942524, 0.14117176, 0.25204984, 0.90152247, 0.83718305,\n",
              "        0.9282671 , 0.35077798, 0.64056154]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **합성곱 계층 구현하기**"
      ],
      "metadata": {
        "id": "n4LzMZPdp7Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# im2col(image to column) 함수 구현\n",
        "\n",
        "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
        "    \"\"\"다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
        "    filter_h : 필터의 높이\n",
        "    filter_w : 필터의 너비\n",
        "    stride : 스트라이드\n",
        "    pad : 패딩\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    col : 2차원 배열\n",
        "    \"\"\"\n",
        "    N, C, H, W = input_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\n",
        "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col"
      ],
      "metadata": {
        "id": "GS0rRY6xrJ3x"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# im2col 사용해보기\n",
        "\n",
        "x1 = np.random.rand(1, 3, 7, 7) # (데이터 수, 채널수, 높이, 너비)\n",
        "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
        "print(col1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLfaCFrpra4w",
        "outputId": "119ed02a-88ab-4ca8-a05a-b1620baf2a6f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "im2col을 통해 4차원 배열의 데이터가 2차원 행렬로 출력된 것을 알 수 있다.\n",
        "\n",
        "따라서 im2col은 메모리를 더 많이 소비하는 단점이 있지만서도 행렬 계산에 고도로 최적화되엉 있는 라이브러리들을 활용해 큰 행렬의 곱셈을 빠르게 계산할 수 있다."
      ],
      "metadata": {
        "id": "_XgGk7P5r-qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = np.random.rand(10, 3, 7, 7) # 데이터 10개\n",
        "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
        "print(col2.shape) # (90,75)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ABBfKBQr5SG",
        "outputId": "54e9948b-f96c-4811-e85d-644fbe941e68"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(90, 75)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2번째 차원의 원소는 75개이다. 이 값은 필터의 원소 수와 같다(채널3개, 5 X 5 데이터)\n",
        "\n",
        "또한, 배치크기가 1에서 10이 되면 마찬가지로 1번째 차원의 원소 또한 10배가 된다."
      ],
      "metadata": {
        "id": "Y__3hZ4-sZcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 합성곱 계층 구현\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = int(1+ (W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T # 필터 전개 ex) (10, 3, 5, 5) 형상의 데이터의 원소 수는 총 750개 이므로, reshape(10, -1)을 호출하면 750개의 원소를 10묶음으로, (10, 75)인 배열로 만들어 준다.\n",
        "        out = np.dot(col, col_W) + self.b\n",
        "\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2) # 출력데이터를 적절한 형상으로 바꿔준다. 인덱스로 축의 순서를 변경한다.\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "k3JaDA-Es_Hj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "im2col로 전개한 덕분에 완전연결 계층의 Affine 계층과 거의 똑같이 구현할 수 있다.\n",
        "\n",
        "합성곱 계층의 역전파에서 주의할 점은 im2col을 역으로 처리하는 col2im 함수를 사용해야 한다."
      ],
      "metadata": {
        "id": "34kgNSsctBrT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **풀링(pooling) 계층 구현하기**"
      ],
      "metadata": {
        "id": "jtdZyNTQvRWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "풀링의 경우엔 채널 쪽이 독립적이기 때문에 풀링 적용 영역을 채널마다 독립적으로 전개한다."
      ],
      "metadata": {
        "id": "f3uMP-rLvX7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        # 전개 (1)\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h * self.pool_w)\n",
        "\n",
        "        # 최댓값 (2)\n",
        "        out = np.max(col, axis=1) # 입력 x의 1번째 차원의 축마다 최댓값을 구한다. **주의 : 2차원 배열(행렬)에서 axis=1은 행 방향을 뜻한다.\n",
        "\n",
        "        # 성형 (3)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "QVfYxRo4v0RW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "앞서 보았듯이 풀링 계층 구현은 다음과 같이 세 단계로 진행된다.\n",
        "\n",
        "1. 입력 데이터를 전개한다.\n",
        "2. 행별 최댓값을 구한다.\n",
        "3. 적절한 모양으로 성형한다."
      ],
      "metadata": {
        "id": "CDSsZOCLwMKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN 구현하기**"
      ],
      "metadata": {
        "id": "tOrlXX5mwz85"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* input_dim - 입력 데이터(채널 수, 높이, 너비)의 차원\n",
        "* conv_param = 합성곱 계층의 하이퍼파라미터(딕셔너리). 딕셔너리의 키는 다음과 같다.\n",
        "    - filter_num - 필터수\n",
        "    - filter_size - 필터크기\n",
        "    - stride - 스트라이드\n",
        "    - pad - 패딩\n",
        "* hidden_size - 은닉층(완전연결)의 뉴런 수\n",
        "* output_size - 출력층(완전연결)의 뉴런 수\n",
        "* weight_init_std - 초기화 때의 가중치 표준편차\n",
        "    * 'relu'나 'he'로 지정하면 'He 초깃값'\n",
        "    * 'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정"
      ],
      "metadata": {
        "id": "egjWJNI3zHdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/밑바닥부터시작하는딥러닝/deep-learning-from-scratch-master')\n",
        "import pickle\n",
        "from collections import OrderedDict\n",
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "\n",
        "class SimpleConvNet:\n",
        "    \n",
        "    # 초기화 인수로 주어진 CNN의 하이퍼파라미터를 나중에 쓰기 쉽도록 딕셔너리에서 꺼낸다. 그리고 합성곱 계층의 출력 크기를 계산한다.\n",
        "    def __init(self, input_dim = (1, 28, 28), \n",
        "               conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "               hidden_size = 100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 매개변수를 초기화 한다.\n",
        "        self.params = {}\n",
        "        # 1번째 층\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = np.zeros(filter_num)\n",
        "        # 2번째 층\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = np.zeros(hidden_size)\n",
        "        # 3번째 층\n",
        "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = np.zeros(output_size)\n",
        "        \n",
        "        # CNN을 구성하는 계층들 생성\n",
        "        self.layers = OrderedDict() # 순서가 있는 딕셔너리에 저장\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss() # 마지막 계층은 last_layer라는 별도 변수에 저장\n",
        "\n",
        "    # 추론과 손실 함수 구하기\n",
        "    def predict(self, x):\n",
        "        for lyaer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    # 오차역전파법으로 기울기 구하기\n",
        "    def gradient(self, x, t):\n",
        "        # 순전파\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # 역전파\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.revers()\n",
        "        for layer in layers:\n",
        "            dout = lyaer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "4XMZVzHfxZ94"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SimpleCovNet으로 MINST 데이터셋 학습**"
      ],
      "metadata": {
        "id": "1EBnOLzP15jD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.mnist import load_mnist\n",
        "from ch07.simple_convnet import SimpleConvNet\n",
        "from common.trainer import Trainer\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q9OY16Tf3LDH",
        "outputId": "921812bf-375f-4659-de37-5f6c21bf55a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:2.298316411642773\n",
            "=== epoch:1, train acc:0.272, test acc:0.282 ===\n",
            "train loss:2.295530437022263\n",
            "train loss:2.2937045864137566\n",
            "train loss:2.288125016465555\n",
            "train loss:2.277892195192799\n",
            "train loss:2.2684331972821408\n",
            "train loss:2.256919096146327\n",
            "train loss:2.2241714965689043\n",
            "train loss:2.2130246165046037\n",
            "train loss:2.1943517530601335\n",
            "train loss:2.1486750134225163\n",
            "train loss:2.116214350126084\n",
            "train loss:2.0904741022943854\n",
            "train loss:2.0367826349485325\n",
            "train loss:1.9726251022856462\n",
            "train loss:1.850969990177189\n",
            "train loss:1.7933364565853598\n",
            "train loss:1.7720516081536324\n",
            "train loss:1.7203081644505125\n",
            "train loss:1.6830183488248533\n",
            "train loss:1.4923693359119325\n",
            "train loss:1.5205874313682963\n",
            "train loss:1.345597306744823\n",
            "train loss:1.385076303512217\n",
            "train loss:1.2371742944634352\n",
            "train loss:1.1708484025254458\n",
            "train loss:1.1052390719085652\n",
            "train loss:1.153997637282493\n",
            "train loss:1.0462833957463196\n",
            "train loss:0.8725802435596249\n",
            "train loss:0.8546015985231195\n",
            "train loss:0.9133529813007978\n",
            "train loss:0.8065572574292742\n",
            "train loss:0.8915052482615067\n",
            "train loss:0.9472310025658475\n",
            "train loss:0.7814702429790803\n",
            "train loss:0.640644536062202\n",
            "train loss:0.6131009194096287\n",
            "train loss:0.7119390368311643\n",
            "train loss:0.8501015696206529\n",
            "train loss:0.6044926113888679\n",
            "train loss:0.6420978414363628\n",
            "train loss:0.683185743334746\n",
            "train loss:0.4990372222779871\n",
            "train loss:0.49450642893311064\n",
            "train loss:0.47934198888835694\n",
            "train loss:0.4117829002804739\n",
            "train loss:0.6492633225525988\n",
            "train loss:0.6741333504715401\n",
            "train loss:0.6976606586995441\n",
            "train loss:0.4236984352062361\n",
            "=== epoch:2, train acc:0.824, test acc:0.794 ===\n",
            "train loss:0.6045743502962317\n",
            "train loss:0.5851684284669688\n",
            "train loss:0.42654205810350154\n",
            "train loss:0.45616465720442967\n",
            "train loss:0.4879090488251612\n",
            "train loss:0.4697291736203557\n",
            "train loss:0.4695421181733409\n",
            "train loss:0.5481909614917329\n",
            "train loss:0.5344330835960226\n",
            "train loss:0.41034805803774677\n",
            "train loss:0.452791768670906\n",
            "train loss:0.422142977217255\n",
            "train loss:0.4792591802511383\n",
            "train loss:0.48723994252983\n",
            "train loss:0.4395673724084233\n",
            "train loss:0.3890270464923587\n",
            "train loss:0.4520561131293922\n",
            "train loss:0.5351353803669067\n",
            "train loss:0.41737201403155966\n",
            "train loss:0.36900817304503714\n",
            "train loss:0.4398236404650026\n",
            "train loss:0.44329120475840367\n",
            "train loss:0.39906196575240704\n",
            "train loss:0.5468642019765971\n",
            "train loss:0.4067994950018101\n",
            "train loss:0.4414453497257786\n",
            "train loss:0.5355069044926534\n",
            "train loss:0.4995749162516502\n",
            "train loss:0.3663588612448223\n",
            "train loss:0.49461231780850057\n",
            "train loss:0.45719118594498115\n",
            "train loss:0.403295280420941\n",
            "train loss:0.36687135416290834\n",
            "train loss:0.37058463606882713\n",
            "train loss:0.37719272789494857\n",
            "train loss:0.34795634783164325\n",
            "train loss:0.4040693907038571\n",
            "train loss:0.5097570966743408\n",
            "train loss:0.3980250869842358\n",
            "train loss:0.19529024235408265\n",
            "train loss:0.4375674631971165\n",
            "train loss:0.235939239403402\n",
            "train loss:0.32520996284648246\n",
            "train loss:0.3529470855590429\n",
            "train loss:0.24464298653969357\n",
            "train loss:0.2551186612357918\n",
            "train loss:0.30650152133948727\n",
            "train loss:0.5696184258955841\n",
            "train loss:0.4206492524685914\n",
            "train loss:0.3478593388040164\n",
            "=== epoch:3, train acc:0.887, test acc:0.872 ===\n",
            "train loss:0.3337157183309427\n",
            "train loss:0.4310626556538055\n",
            "train loss:0.3663677398183472\n",
            "train loss:0.440593171516095\n",
            "train loss:0.37267385741180364\n",
            "train loss:0.397671034794669\n",
            "train loss:0.3617027611747515\n",
            "train loss:0.26491097064741775\n",
            "train loss:0.3045069043382751\n",
            "train loss:0.22443034260627917\n",
            "train loss:0.30499528798646586\n",
            "train loss:0.42742351648016763\n",
            "train loss:0.34440419430149666\n",
            "train loss:0.43295622551271334\n",
            "train loss:0.30807189335666973\n",
            "train loss:0.30378617120140716\n",
            "train loss:0.33630875533275306\n",
            "train loss:0.23949246995228232\n",
            "train loss:0.28865284038915295\n",
            "train loss:0.21838264917131067\n",
            "train loss:0.55558521825366\n",
            "train loss:0.23759345290457887\n",
            "train loss:0.33306004766662434\n",
            "train loss:0.36576967283639644\n",
            "train loss:0.4599121731871316\n",
            "train loss:0.36827392704009915\n",
            "train loss:0.3366064667749757\n",
            "train loss:0.2707147272398074\n",
            "train loss:0.33253008290746644\n",
            "train loss:0.41964343381828634\n",
            "train loss:0.25370056407964\n",
            "train loss:0.3536671567160128\n",
            "train loss:0.2669799972481625\n",
            "train loss:0.34768725464882294\n",
            "train loss:0.2187399985017976\n",
            "train loss:0.4735086326252162\n",
            "train loss:0.39669894809580514\n",
            "train loss:0.26314504461321436\n",
            "train loss:0.3947352222256801\n",
            "train loss:0.31538552272253395\n",
            "train loss:0.24818494625915452\n",
            "train loss:0.5566907130541677\n",
            "train loss:0.3170526168462153\n",
            "train loss:0.2028107904879862\n",
            "train loss:0.3757199716492298\n",
            "train loss:0.32896047830555736\n",
            "train loss:0.30986158032662237\n",
            "train loss:0.3562749399172121\n",
            "train loss:0.3072211689609896\n",
            "train loss:0.3530921683000427\n",
            "=== epoch:4, train acc:0.875, test acc:0.848 ===\n",
            "train loss:0.3321577337725449\n",
            "train loss:0.35609205474613076\n",
            "train loss:0.3938043894631255\n",
            "train loss:0.37971076715581303\n",
            "train loss:0.3204393510749252\n",
            "train loss:0.3083384293959231\n",
            "train loss:0.23787633604839353\n",
            "train loss:0.23123196298472695\n",
            "train loss:0.2363238630266622\n",
            "train loss:0.27246257282568975\n",
            "train loss:0.3136965465088608\n",
            "train loss:0.323744484295833\n",
            "train loss:0.15106344526289686\n",
            "train loss:0.28754731416251866\n",
            "train loss:0.29685059885480786\n",
            "train loss:0.19507210169419906\n",
            "train loss:0.297811043475857\n",
            "train loss:0.16484108714410722\n",
            "train loss:0.13646853685290247\n",
            "train loss:0.34627730687354247\n",
            "train loss:0.2584232522862153\n",
            "train loss:0.23038385026644512\n",
            "train loss:0.20888988933677063\n",
            "train loss:0.31602668939790096\n",
            "train loss:0.1625707944808295\n",
            "train loss:0.21273705903072535\n",
            "train loss:0.19591938716380417\n",
            "train loss:0.36245902454295836\n",
            "train loss:0.24005112289682654\n",
            "train loss:0.2793079808119533\n",
            "train loss:0.1562360933638938\n",
            "train loss:0.2875449923537882\n",
            "train loss:0.3061479388060984\n",
            "train loss:0.30967982063911154\n",
            "train loss:0.22340972565504147\n",
            "train loss:0.3177125229475305\n",
            "train loss:0.11488571642395633\n",
            "train loss:0.1580142022692859\n",
            "train loss:0.18101694917563693\n",
            "train loss:0.11077606028671734\n",
            "train loss:0.1032376850634848\n",
            "train loss:0.30434464468675004\n",
            "train loss:0.20705353078569247\n",
            "train loss:0.21450044341007515\n",
            "train loss:0.28282878417256824\n",
            "train loss:0.22876494139316111\n",
            "train loss:0.1937963819545942\n",
            "train loss:0.3422217557979299\n",
            "train loss:0.20058578314695208\n",
            "train loss:0.15442539397999375\n",
            "=== epoch:5, train acc:0.911, test acc:0.896 ===\n",
            "train loss:0.16004156834418296\n",
            "train loss:0.25119227851223036\n",
            "train loss:0.3302469174490294\n",
            "train loss:0.3082270408422373\n",
            "train loss:0.2651479222588769\n",
            "train loss:0.22870575077499272\n",
            "train loss:0.18332421008862043\n",
            "train loss:0.317178283715679\n",
            "train loss:0.2400616664795769\n",
            "train loss:0.15291488037458206\n",
            "train loss:0.23062225281007076\n",
            "train loss:0.2787153607521191\n",
            "train loss:0.31491771616361924\n",
            "train loss:0.22294457373189505\n",
            "train loss:0.15525875012999166\n",
            "train loss:0.3125538912570539\n",
            "train loss:0.10942259107262586\n",
            "train loss:0.2329201983761128\n",
            "train loss:0.3004393240491675\n",
            "train loss:0.24517862041018185\n",
            "train loss:0.25209966447073606\n",
            "train loss:0.22668742889139146\n",
            "train loss:0.1819038333844888\n",
            "train loss:0.23957113760724058\n",
            "train loss:0.17876716491199385\n",
            "train loss:0.18438691590061254\n",
            "train loss:0.21047729845147692\n",
            "train loss:0.20880313807091674\n",
            "train loss:0.36467927668126726\n",
            "train loss:0.18812177311303885\n",
            "train loss:0.09717027557735129\n",
            "train loss:0.3849764361113906\n",
            "train loss:0.3451063556610798\n",
            "train loss:0.1785658253863554\n",
            "train loss:0.2610228388419457\n",
            "train loss:0.2845656405210377\n",
            "train loss:0.25034892648479784\n",
            "train loss:0.08529269615555467\n",
            "train loss:0.2016981993035158\n",
            "train loss:0.28974216450318374\n",
            "train loss:0.15503729594570026\n",
            "train loss:0.36411505782307024\n",
            "train loss:0.3231520848418633\n",
            "train loss:0.3072087392931293\n",
            "train loss:0.07556006495564808\n",
            "train loss:0.19737695465700175\n",
            "train loss:0.33208601460038023\n",
            "train loss:0.17802398973761385\n",
            "train loss:0.1701759233149663\n",
            "train loss:0.3110394953415517\n",
            "=== epoch:6, train acc:0.924, test acc:0.904 ===\n",
            "train loss:0.27247777704414744\n",
            "train loss:0.11753309895074278\n",
            "train loss:0.2968363526244139\n",
            "train loss:0.19728199215811174\n",
            "train loss:0.16256112102576725\n",
            "train loss:0.24890448309733937\n",
            "train loss:0.13401950049920672\n",
            "train loss:0.17499048330183076\n",
            "train loss:0.2006338845606189\n",
            "train loss:0.15210871507874402\n",
            "train loss:0.18004444692764335\n",
            "train loss:0.22847795354052466\n",
            "train loss:0.15437292653042867\n",
            "train loss:0.21645329657933896\n",
            "train loss:0.1527864322909409\n",
            "train loss:0.39536978906257536\n",
            "train loss:0.1592850985248752\n",
            "train loss:0.1400660215714327\n",
            "train loss:0.23063451089964027\n",
            "train loss:0.31075596238653214\n",
            "train loss:0.24217156025419473\n",
            "train loss:0.21696509697836244\n",
            "train loss:0.11710858592483987\n",
            "train loss:0.25802231001825127\n",
            "train loss:0.22502061090741163\n",
            "train loss:0.18599917346080805\n",
            "train loss:0.15869361219706463\n",
            "train loss:0.20979760180602658\n",
            "train loss:0.26860872091022187\n",
            "train loss:0.12542354731233846\n",
            "train loss:0.1584869399228757\n",
            "train loss:0.1724714837206649\n",
            "train loss:0.10280285752310095\n",
            "train loss:0.156276891184333\n",
            "train loss:0.2284188131956032\n",
            "train loss:0.13460000360545127\n",
            "train loss:0.22929729204072732\n",
            "train loss:0.14301787773732047\n",
            "train loss:0.26854998144282566\n",
            "train loss:0.09385458177344677\n",
            "train loss:0.1530349861470553\n",
            "train loss:0.27213470165931086\n",
            "train loss:0.2244992874348652\n",
            "train loss:0.08973072809305116\n",
            "train loss:0.09421109481631597\n",
            "train loss:0.19297742537902202\n",
            "train loss:0.18231283694665137\n",
            "train loss:0.1380059819063625\n",
            "train loss:0.168611211736182\n",
            "train loss:0.15144393800958444\n",
            "=== epoch:7, train acc:0.936, test acc:0.916 ===\n",
            "train loss:0.14075515309403847\n",
            "train loss:0.06572558543829427\n",
            "train loss:0.152804299213976\n",
            "train loss:0.18720750561535773\n",
            "train loss:0.13604741540284054\n",
            "train loss:0.21339966052664547\n",
            "train loss:0.12167602792199418\n",
            "train loss:0.08102797862732677\n",
            "train loss:0.14529841090144194\n",
            "train loss:0.10985436517261356\n",
            "train loss:0.07663804427819787\n",
            "train loss:0.1317000002643409\n",
            "train loss:0.14914978902870465\n",
            "train loss:0.11447149224025609\n",
            "train loss:0.18540163000133136\n",
            "train loss:0.12526681853205884\n",
            "train loss:0.14643607410736964\n",
            "train loss:0.17987182945509952\n",
            "train loss:0.28861099979167887\n",
            "train loss:0.1329429144355128\n",
            "train loss:0.11728641361355832\n",
            "train loss:0.153379171219357\n",
            "train loss:0.305642592511039\n",
            "train loss:0.1665419941371096\n",
            "train loss:0.12046533845196178\n",
            "train loss:0.14994447256981083\n",
            "train loss:0.07873921748436158\n",
            "train loss:0.312600814171461\n",
            "train loss:0.06400882167160044\n",
            "train loss:0.08721921191715708\n",
            "train loss:0.06604720752591187\n",
            "train loss:0.11189723336415425\n",
            "train loss:0.10648433846341325\n",
            "train loss:0.2479591380758411\n",
            "train loss:0.08826978368546884\n",
            "train loss:0.09920780514440868\n",
            "train loss:0.09398882933021575\n",
            "train loss:0.14455269093199838\n",
            "train loss:0.15883490339442644\n",
            "train loss:0.2307548298944998\n",
            "train loss:0.13732805763037012\n",
            "train loss:0.10932587951973166\n",
            "train loss:0.08517741503176468\n",
            "train loss:0.14338428646048448\n",
            "train loss:0.2116268337186818\n",
            "train loss:0.12614231817521585\n",
            "train loss:0.22895276190957875\n",
            "train loss:0.2417307710000911\n",
            "train loss:0.19972870259494918\n",
            "train loss:0.10551682051619378\n",
            "=== epoch:8, train acc:0.942, test acc:0.917 ===\n",
            "train loss:0.16599284444979265\n",
            "train loss:0.19116004957524663\n",
            "train loss:0.1286631123280579\n",
            "train loss:0.09756078256155189\n",
            "train loss:0.2319644750933223\n",
            "train loss:0.1813936154136214\n",
            "train loss:0.13542108600099348\n",
            "train loss:0.29710045820956515\n",
            "train loss:0.16677863968911166\n",
            "train loss:0.16977413014167872\n",
            "train loss:0.1734772948876335\n",
            "train loss:0.2044336946218407\n",
            "train loss:0.11671396368277398\n",
            "train loss:0.1072995047949679\n",
            "train loss:0.13432177611025553\n",
            "train loss:0.21304647073970412\n",
            "train loss:0.1588661691822043\n",
            "train loss:0.07529989224208915\n",
            "train loss:0.08129148084797885\n",
            "train loss:0.14436543913368624\n",
            "train loss:0.1766755513625964\n",
            "train loss:0.15171587599070452\n",
            "train loss:0.16423512531856946\n",
            "train loss:0.12667050716339426\n",
            "train loss:0.08976541063225542\n",
            "train loss:0.14446007073898395\n",
            "train loss:0.14278826754820736\n",
            "train loss:0.08851677229288665\n",
            "train loss:0.1075770822439604\n",
            "train loss:0.13614823102550105\n",
            "train loss:0.22181671909530756\n",
            "train loss:0.13021852082428156\n",
            "train loss:0.1571786589120262\n",
            "train loss:0.20122583000805097\n",
            "train loss:0.05216318070672745\n",
            "train loss:0.16571950311958175\n",
            "train loss:0.15736232799377364\n",
            "train loss:0.16375427985945706\n",
            "train loss:0.07593022906209242\n",
            "train loss:0.10532708939923684\n",
            "train loss:0.10784052401751353\n",
            "train loss:0.1320900710351059\n",
            "train loss:0.13131507023944694\n",
            "train loss:0.19262867849136472\n",
            "train loss:0.10828940011747794\n",
            "train loss:0.09049320058171656\n",
            "train loss:0.30971684747705125\n",
            "train loss:0.14076842675438517\n",
            "train loss:0.1481007102672298\n",
            "train loss:0.159881454446886\n",
            "=== epoch:9, train acc:0.947, test acc:0.926 ===\n",
            "train loss:0.1636136900031668\n",
            "train loss:0.15031996614119575\n",
            "train loss:0.07688442853616768\n",
            "train loss:0.14510806637731058\n",
            "train loss:0.1783445351400484\n",
            "train loss:0.0799449010069885\n",
            "train loss:0.11832753497888071\n",
            "train loss:0.14110946355254897\n",
            "train loss:0.19007160568234446\n",
            "train loss:0.09051701718404231\n",
            "train loss:0.14723690956755683\n",
            "train loss:0.07145726015551673\n",
            "train loss:0.1369602521873381\n",
            "train loss:0.09434250795335014\n",
            "train loss:0.0996938475957745\n",
            "train loss:0.04361612596379331\n",
            "train loss:0.07504800854189368\n",
            "train loss:0.19827924288440343\n",
            "train loss:0.10710323958530195\n",
            "train loss:0.1030355047362188\n",
            "train loss:0.15981249090348873\n",
            "train loss:0.09237226247983685\n",
            "train loss:0.0968076193038214\n",
            "train loss:0.14432651828692303\n",
            "train loss:0.12130560101533304\n",
            "train loss:0.09759869503900041\n",
            "train loss:0.2401663364097202\n",
            "train loss:0.097497540281466\n",
            "train loss:0.08963712169344429\n",
            "train loss:0.105620837218766\n",
            "train loss:0.1203126306249288\n",
            "train loss:0.08984490511285628\n",
            "train loss:0.12559340903532992\n",
            "train loss:0.16339538661816672\n",
            "train loss:0.13258144607886949\n",
            "train loss:0.26801357857506847\n",
            "train loss:0.09358159038091372\n",
            "train loss:0.05754728254350974\n",
            "train loss:0.08026654803687108\n",
            "train loss:0.14745815322661712\n",
            "train loss:0.1210488289149928\n",
            "train loss:0.056762521812544264\n",
            "train loss:0.07245252989094304\n",
            "train loss:0.060050093604823225\n",
            "train loss:0.10363181842150603\n",
            "train loss:0.14375064245289967\n",
            "train loss:0.03381693842898847\n",
            "train loss:0.030761462450435345\n",
            "train loss:0.12359581866609336\n",
            "train loss:0.1591175833227205\n",
            "=== epoch:10, train acc:0.961, test acc:0.932 ===\n",
            "train loss:0.06856219839672507\n",
            "train loss:0.11721834099892142\n",
            "train loss:0.032214144562927496\n",
            "train loss:0.09407563709827173\n",
            "train loss:0.14443137206571313\n",
            "train loss:0.08006793061879477\n",
            "train loss:0.05827804867905277\n",
            "train loss:0.12512637916378583\n",
            "train loss:0.09878337314270945\n",
            "train loss:0.13769776514838944\n",
            "train loss:0.12820190584637536\n",
            "train loss:0.13581169347321392\n",
            "train loss:0.10748305714289354\n",
            "train loss:0.144633768925563\n",
            "train loss:0.16903639018901606\n",
            "train loss:0.10853971398784765\n",
            "train loss:0.07209182380314885\n",
            "train loss:0.12349247099718376\n",
            "train loss:0.10249898996597277\n",
            "train loss:0.07458754246479202\n",
            "train loss:0.1424666788072391\n",
            "train loss:0.03446225620700261\n",
            "train loss:0.06340650641871917\n",
            "train loss:0.08237431259353753\n",
            "train loss:0.12642819462641863\n",
            "train loss:0.13297721795821904\n",
            "train loss:0.11343268353676324\n",
            "train loss:0.079757852842571\n",
            "train loss:0.22744946900928195\n",
            "train loss:0.10345671435249203\n",
            "train loss:0.04199405016804966\n",
            "train loss:0.09375687984973453\n",
            "train loss:0.06745188681116278\n",
            "train loss:0.04682536327189678\n",
            "train loss:0.10544115715181984\n",
            "train loss:0.05390788942862951\n",
            "train loss:0.13252228936716187\n",
            "train loss:0.06689441811227556\n",
            "train loss:0.04769147866547588\n",
            "train loss:0.09232493692801226\n",
            "train loss:0.09228229210130598\n",
            "train loss:0.1250021968106687\n",
            "train loss:0.051574997384558355\n",
            "train loss:0.11936520264593806\n",
            "train loss:0.14249469792704889\n",
            "train loss:0.04026020015107088\n",
            "train loss:0.11394553887040519\n",
            "train loss:0.10404077987304604\n",
            "train loss:0.09027893292615057\n",
            "train loss:0.05818303452791376\n",
            "=== epoch:11, train acc:0.968, test acc:0.949 ===\n",
            "train loss:0.046726057738555496\n",
            "train loss:0.03918797509090427\n",
            "train loss:0.09807562437204002\n",
            "train loss:0.0907932981415128\n",
            "train loss:0.11435440387967788\n",
            "train loss:0.08442526169416695\n",
            "train loss:0.07354114675483803\n",
            "train loss:0.0688290386906962\n",
            "train loss:0.1167818492046201\n",
            "train loss:0.041170640502312575\n",
            "train loss:0.09183351179468018\n",
            "train loss:0.10783597805648971\n",
            "train loss:0.11843591912756106\n",
            "train loss:0.11379569559882562\n",
            "train loss:0.08011338617300044\n",
            "train loss:0.0977645194013657\n",
            "train loss:0.043420621825609185\n",
            "train loss:0.08140591821156556\n",
            "train loss:0.10529022765896137\n",
            "train loss:0.03292144397117134\n",
            "train loss:0.09470992855224943\n",
            "train loss:0.09841981191720349\n",
            "train loss:0.06358344122007538\n",
            "train loss:0.16987056108352422\n",
            "train loss:0.05255429872236992\n",
            "train loss:0.164308480013514\n",
            "train loss:0.09940659478769431\n",
            "train loss:0.061329716674393894\n",
            "train loss:0.09017450622965682\n",
            "train loss:0.07772740207456334\n",
            "train loss:0.09425643952154336\n",
            "train loss:0.10898986186405817\n",
            "train loss:0.06928157320616515\n",
            "train loss:0.09732107585894115\n",
            "train loss:0.07776633919569517\n",
            "train loss:0.12039535806289138\n",
            "train loss:0.04657615460408698\n",
            "train loss:0.0845458822813839\n",
            "train loss:0.06724296453379987\n",
            "train loss:0.09818824474384791\n",
            "train loss:0.06313174574303694\n",
            "train loss:0.04907610875563629\n",
            "train loss:0.05211989762924354\n",
            "train loss:0.0718839798252298\n",
            "train loss:0.044584509685445334\n",
            "train loss:0.07460856728326778\n",
            "train loss:0.0677086119110168\n",
            "train loss:0.08647238568578895\n",
            "train loss:0.09162472774377436\n",
            "train loss:0.048330591954894954\n",
            "=== epoch:12, train acc:0.972, test acc:0.946 ===\n",
            "train loss:0.05578294509335221\n",
            "train loss:0.07217548711685348\n",
            "train loss:0.030377561761340828\n",
            "train loss:0.034119275493322714\n",
            "train loss:0.07888674900374153\n",
            "train loss:0.062069057648465875\n",
            "train loss:0.0819572468410198\n",
            "train loss:0.056556597840076245\n",
            "train loss:0.06618856772169598\n",
            "train loss:0.06022172511791539\n",
            "train loss:0.03994879252858016\n",
            "train loss:0.09934148714788937\n",
            "train loss:0.06920891741230638\n",
            "train loss:0.10695107257871354\n",
            "train loss:0.06620220143801153\n",
            "train loss:0.0616448737789591\n",
            "train loss:0.07340905695010488\n",
            "train loss:0.060971920237102475\n",
            "train loss:0.03167389710733505\n",
            "train loss:0.06610872174192722\n",
            "train loss:0.04377418652081613\n",
            "train loss:0.055654008029897666\n",
            "train loss:0.07185830174198769\n",
            "train loss:0.03288642305831411\n",
            "train loss:0.04623108609890789\n",
            "train loss:0.051961591409622344\n",
            "train loss:0.0455367060288743\n",
            "train loss:0.08204308343444859\n",
            "train loss:0.1178163772220162\n",
            "train loss:0.08204275111913102\n",
            "train loss:0.07500433160942081\n",
            "train loss:0.045263222559867325\n",
            "train loss:0.07154100864098971\n",
            "train loss:0.04630319579134102\n",
            "train loss:0.054620069205363574\n",
            "train loss:0.03677984154605931\n",
            "train loss:0.06662207231193869\n",
            "train loss:0.10381048358850041\n",
            "train loss:0.07384123531031782\n",
            "train loss:0.10319996065913552\n",
            "train loss:0.038827517879745026\n",
            "train loss:0.08876734268312748\n",
            "train loss:0.01897895320119565\n",
            "train loss:0.07267787360586758\n",
            "train loss:0.07306078150356837\n",
            "train loss:0.04202880789991119\n",
            "train loss:0.08484878893173456\n",
            "train loss:0.06368590176960442\n",
            "train loss:0.06571871414643696\n",
            "train loss:0.024068950572347605\n",
            "=== epoch:13, train acc:0.977, test acc:0.948 ===\n",
            "train loss:0.04527472872295116\n",
            "train loss:0.022534068518894782\n",
            "train loss:0.07838865745708928\n",
            "train loss:0.032795010836393176\n",
            "train loss:0.03981017135930516\n",
            "train loss:0.07125721996659014\n",
            "train loss:0.06256474096316042\n",
            "train loss:0.05282766340908482\n",
            "train loss:0.0527466193884491\n",
            "train loss:0.09527213853821567\n",
            "train loss:0.10767336881289974\n",
            "train loss:0.07847893426767744\n",
            "train loss:0.09635421259333579\n",
            "train loss:0.12561925831791498\n",
            "train loss:0.04366593583692326\n",
            "train loss:0.10940375551467567\n",
            "train loss:0.028377492385455016\n",
            "train loss:0.03511723203474642\n",
            "train loss:0.06928483475088511\n",
            "train loss:0.025645890004441858\n",
            "train loss:0.05220605384786817\n",
            "train loss:0.06004248518756846\n",
            "train loss:0.07804269892820254\n",
            "train loss:0.17166228485766458\n",
            "train loss:0.029690913363120085\n",
            "train loss:0.11911794881375631\n",
            "train loss:0.04225419477400668\n",
            "train loss:0.07904594124524945\n",
            "train loss:0.05323097529588877\n",
            "train loss:0.020584057719638144\n",
            "train loss:0.04213412137781491\n",
            "train loss:0.03210408249289364\n",
            "train loss:0.03442361026047433\n",
            "train loss:0.04197605516073485\n",
            "train loss:0.06246009170221644\n",
            "train loss:0.041294933157657907\n",
            "train loss:0.060374884904105086\n",
            "train loss:0.07128373956799477\n",
            "train loss:0.056205547026420326\n",
            "train loss:0.04437755886690741\n",
            "train loss:0.059774426601206576\n",
            "train loss:0.05086566713904544\n",
            "train loss:0.05536236669749073\n",
            "train loss:0.018844606280741288\n",
            "train loss:0.06273445843785087\n",
            "train loss:0.06371921042307398\n",
            "train loss:0.058464717349503384\n",
            "train loss:0.06370417211040097\n",
            "train loss:0.07513406219672185\n",
            "train loss:0.0675654949453836\n",
            "=== epoch:14, train acc:0.977, test acc:0.952 ===\n",
            "train loss:0.04036969770130192\n",
            "train loss:0.0985063986663636\n",
            "train loss:0.05846590989150236\n",
            "train loss:0.06953453994830357\n",
            "train loss:0.12383838529758899\n",
            "train loss:0.020671917950931505\n",
            "train loss:0.04007649207864822\n",
            "train loss:0.03929028241991145\n",
            "train loss:0.0456469069684323\n",
            "train loss:0.09616777559212367\n",
            "train loss:0.04850933277859793\n",
            "train loss:0.012048452587107922\n",
            "train loss:0.037011244231457054\n",
            "train loss:0.034759161059032095\n",
            "train loss:0.05147366734247524\n",
            "train loss:0.069571513425062\n",
            "train loss:0.01510865864663559\n",
            "train loss:0.023966546454530627\n",
            "train loss:0.028007499178480423\n",
            "train loss:0.055766167811278786\n",
            "train loss:0.07088198775142086\n",
            "train loss:0.04865248786190697\n",
            "train loss:0.04776578480944229\n",
            "train loss:0.02155472980102298\n",
            "train loss:0.031069474395889495\n",
            "train loss:0.04857862105582525\n",
            "train loss:0.06210518449920949\n",
            "train loss:0.06446142919658744\n",
            "train loss:0.07008393496630032\n",
            "train loss:0.056758816104113576\n",
            "train loss:0.04817031243978012\n",
            "train loss:0.012467492076042861\n",
            "train loss:0.03640051203715938\n",
            "train loss:0.14128354692543155\n",
            "train loss:0.058861008705920065\n",
            "train loss:0.03433356493203263\n",
            "train loss:0.06356808385092588\n",
            "train loss:0.041314357772491345\n",
            "train loss:0.05132175118431067\n",
            "train loss:0.08291021636154974\n",
            "train loss:0.03595719036585602\n",
            "train loss:0.08065625071580486\n",
            "train loss:0.05262828471089968\n",
            "train loss:0.04140702617921775\n",
            "train loss:0.14329199077723165\n",
            "train loss:0.06270080341720716\n",
            "train loss:0.026540620060062373\n",
            "train loss:0.023453389807538826\n",
            "train loss:0.13985218433743485\n",
            "train loss:0.06737714731815694\n",
            "=== epoch:15, train acc:0.981, test acc:0.955 ===\n",
            "train loss:0.0579604654803748\n",
            "train loss:0.07421177962407703\n",
            "train loss:0.06751062581852803\n",
            "train loss:0.03725103572003773\n",
            "train loss:0.017835060686400027\n",
            "train loss:0.04730826073815094\n",
            "train loss:0.052465526338524054\n",
            "train loss:0.03014564474875723\n",
            "train loss:0.04349395156194176\n",
            "train loss:0.02028341677087682\n",
            "train loss:0.03521068624442742\n",
            "train loss:0.06957376581743213\n",
            "train loss:0.06382144933425454\n",
            "train loss:0.07973808088354482\n",
            "train loss:0.03403342217039993\n",
            "train loss:0.023888592560464484\n",
            "train loss:0.041281403060120014\n",
            "train loss:0.03990439661698804\n",
            "train loss:0.0339182768932446\n",
            "train loss:0.046904957480956445\n",
            "train loss:0.06776994831137352\n",
            "train loss:0.06405960399916934\n",
            "train loss:0.11169432622966705\n",
            "train loss:0.05418477045712134\n",
            "train loss:0.05836874143601482\n",
            "train loss:0.02558041099656829\n",
            "train loss:0.035329065628505664\n",
            "train loss:0.06239317153840648\n",
            "train loss:0.03750876996567107\n",
            "train loss:0.02897224100452076\n",
            "train loss:0.04192505900571109\n",
            "train loss:0.027852079684588894\n",
            "train loss:0.01895700203617243\n",
            "train loss:0.041811751383290865\n",
            "train loss:0.04795345402300796\n",
            "train loss:0.02196774315227743\n",
            "train loss:0.09346011986565479\n",
            "train loss:0.03683546874162492\n",
            "train loss:0.05621745426306243\n",
            "train loss:0.03211104787079575\n",
            "train loss:0.04665251500124808\n",
            "train loss:0.0254430207528284\n",
            "train loss:0.030379762718787968\n",
            "train loss:0.04719493774591528\n",
            "train loss:0.03401003617754465\n",
            "train loss:0.039778890582460175\n",
            "train loss:0.024571914331751987\n",
            "train loss:0.018837001324588237\n",
            "train loss:0.021690091994770108\n",
            "train loss:0.013064762981714715\n",
            "=== epoch:16, train acc:0.982, test acc:0.96 ===\n",
            "train loss:0.031238307137091005\n",
            "train loss:0.021377820236767625\n",
            "train loss:0.018956801466569282\n",
            "train loss:0.017175916692095707\n",
            "train loss:0.026803064544661876\n",
            "train loss:0.04130848479704538\n",
            "train loss:0.01939532425828993\n",
            "train loss:0.05621710122035323\n",
            "train loss:0.011300646362481664\n",
            "train loss:0.03459656079373667\n",
            "train loss:0.024008778330092077\n",
            "train loss:0.030075131087652748\n",
            "train loss:0.02167589229265984\n",
            "train loss:0.045669178170264224\n",
            "train loss:0.031829558568979247\n",
            "train loss:0.030581396937110105\n",
            "train loss:0.029633317295228893\n",
            "train loss:0.012812522112072913\n",
            "train loss:0.0470526200791782\n",
            "train loss:0.1031056241603102\n",
            "train loss:0.0398808999816566\n",
            "train loss:0.04985243567208633\n",
            "train loss:0.03263842904936039\n",
            "train loss:0.04484465317824156\n",
            "train loss:0.06090662843462936\n",
            "train loss:0.03158825900813693\n",
            "train loss:0.03294714304325289\n",
            "train loss:0.015529443681489499\n",
            "train loss:0.02177397236126115\n",
            "train loss:0.016384422063363036\n",
            "train loss:0.06749192770199491\n",
            "train loss:0.026536321649632873\n",
            "train loss:0.09063628734224442\n",
            "train loss:0.040413432363364504\n",
            "train loss:0.052667364801985306\n",
            "train loss:0.04602660953965277\n",
            "train loss:0.01670842329971272\n",
            "train loss:0.015251729341124143\n",
            "train loss:0.049263996371929436\n",
            "train loss:0.029260503791545118\n",
            "train loss:0.06067299577591579\n",
            "train loss:0.07587771265197477\n",
            "train loss:0.11081288045039382\n",
            "train loss:0.013175120876051194\n",
            "train loss:0.05102654777633356\n",
            "train loss:0.13507691320875542\n",
            "train loss:0.03499623276249564\n",
            "train loss:0.03814120922995902\n",
            "train loss:0.03613594624808148\n",
            "train loss:0.029924153430210997\n",
            "=== epoch:17, train acc:0.989, test acc:0.962 ===\n",
            "train loss:0.039752272896662416\n",
            "train loss:0.019062038427826784\n",
            "train loss:0.020902097460810176\n",
            "train loss:0.01682614236602846\n",
            "train loss:0.018562959392862264\n",
            "train loss:0.029184297221815623\n",
            "train loss:0.044675848259295575\n",
            "train loss:0.04721698373683755\n",
            "train loss:0.05172951589573865\n",
            "train loss:0.01687316630820152\n",
            "train loss:0.03990917721423799\n",
            "train loss:0.04661271254676149\n",
            "train loss:0.04355499446702935\n",
            "train loss:0.02273142013205474\n",
            "train loss:0.061964772762077035\n",
            "train loss:0.012642558999524163\n",
            "train loss:0.038829701841052784\n",
            "train loss:0.02157916835335415\n",
            "train loss:0.03983599917421767\n",
            "train loss:0.024390319827515418\n",
            "train loss:0.012121099948305436\n",
            "train loss:0.01659911811817668\n",
            "train loss:0.027317238276397898\n",
            "train loss:0.02526786449255323\n",
            "train loss:0.017008561748029866\n",
            "train loss:0.01916712179880018\n",
            "train loss:0.026590462794230038\n",
            "train loss:0.01401402022423231\n",
            "train loss:0.0450759403302621\n",
            "train loss:0.03424954313671046\n",
            "train loss:0.01495371636156092\n",
            "train loss:0.01724076247525502\n",
            "train loss:0.0122981690447718\n",
            "train loss:0.034005442019683155\n",
            "train loss:0.027513515147612896\n",
            "train loss:0.032952380697652536\n",
            "train loss:0.023642756142012656\n",
            "train loss:0.022185689927979032\n",
            "train loss:0.012883341332901541\n",
            "train loss:0.0298317589633738\n",
            "train loss:0.018054789660991666\n",
            "train loss:0.01207073261363907\n",
            "train loss:0.02519248975694399\n",
            "train loss:0.037113645807054334\n",
            "train loss:0.042463679938200355\n",
            "train loss:0.08476665428375521\n",
            "train loss:0.045198743890858914\n",
            "train loss:0.016700056028468274\n",
            "train loss:0.08073757668637312\n",
            "train loss:0.029925487928857755\n",
            "=== epoch:18, train acc:0.987, test acc:0.959 ===\n",
            "train loss:0.018295159589110755\n",
            "train loss:0.046848259535172444\n",
            "train loss:0.01794024333404167\n",
            "train loss:0.02707286809428546\n",
            "train loss:0.013796484897873716\n",
            "train loss:0.01517962751550179\n",
            "train loss:0.018515605039513116\n",
            "train loss:0.023792290288379404\n",
            "train loss:0.04977696233260496\n",
            "train loss:0.029220940704467423\n",
            "train loss:0.01818157677836725\n",
            "train loss:0.03073118825198224\n",
            "train loss:0.035522336709473225\n",
            "train loss:0.03216972736809157\n",
            "train loss:0.014993550952839128\n",
            "train loss:0.025593994172962292\n",
            "train loss:0.04789356887292534\n",
            "train loss:0.015362017491661148\n",
            "train loss:0.018897716322535556\n",
            "train loss:0.016988593319105745\n",
            "train loss:0.017744004048037925\n",
            "train loss:0.019472862202586542\n",
            "train loss:0.013957144290144803\n",
            "train loss:0.00872795777157078\n",
            "train loss:0.03200457617418443\n",
            "train loss:0.03167534982755469\n",
            "train loss:0.06234589741500482\n",
            "train loss:0.010629137624765523\n",
            "train loss:0.016076940561097947\n",
            "train loss:0.023664168198723353\n",
            "train loss:0.012255206172999395\n",
            "train loss:0.020195857104977226\n",
            "train loss:0.03757310134984153\n",
            "train loss:0.025986048512233864\n",
            "train loss:0.009126863919783621\n",
            "train loss:0.04241981336071534\n",
            "train loss:0.036672297603175745\n",
            "train loss:0.03009433378824854\n",
            "train loss:0.022310418225602868\n",
            "train loss:0.03836313685663642\n",
            "train loss:0.030627471424604088\n",
            "train loss:0.015883408762697324\n",
            "train loss:0.012921034435648657\n",
            "train loss:0.02956698385311432\n",
            "train loss:0.03016973888100507\n",
            "train loss:0.031244955852810154\n",
            "train loss:0.024274513121261852\n",
            "train loss:0.017978721712144013\n",
            "train loss:0.08050907975694549\n",
            "train loss:0.012267890411860452\n",
            "=== epoch:19, train acc:0.989, test acc:0.96 ===\n",
            "train loss:0.05336941244090747\n",
            "train loss:0.03676408406857987\n",
            "train loss:0.014982471640276829\n",
            "train loss:0.0241380605056226\n",
            "train loss:0.03359558279420382\n",
            "train loss:0.0209075667510253\n",
            "train loss:0.011974800980320398\n",
            "train loss:0.024512440309535158\n",
            "train loss:0.020419134859550745\n",
            "train loss:0.018287935199351922\n",
            "train loss:0.035439407995786855\n",
            "train loss:0.026827137246200006\n",
            "train loss:0.02068556519463952\n",
            "train loss:0.10321090644876155\n",
            "train loss:0.013436438749067567\n",
            "train loss:0.023622122794690618\n",
            "train loss:0.024900481493652347\n",
            "train loss:0.03738715392921253\n",
            "train loss:0.03165400880870665\n",
            "train loss:0.03778680564838923\n",
            "train loss:0.06352393256029867\n",
            "train loss:0.04163395955470813\n",
            "train loss:0.018264593893954562\n",
            "train loss:0.03115136373434362\n",
            "train loss:0.01844264971607261\n",
            "train loss:0.02347521691739391\n",
            "train loss:0.03595985801299824\n",
            "train loss:0.04610003751625809\n",
            "train loss:0.034219854993578505\n",
            "train loss:0.021215235284628178\n",
            "train loss:0.023564660199050756\n",
            "train loss:0.028991467850154128\n",
            "train loss:0.06700253037631387\n",
            "train loss:0.04486661979106403\n",
            "train loss:0.027868947851777192\n",
            "train loss:0.018891838474949175\n",
            "train loss:0.017730128028062418\n",
            "train loss:0.03285092981354041\n",
            "train loss:0.01767253774450464\n",
            "train loss:0.020565706778235788\n",
            "train loss:0.01174128176280951\n",
            "train loss:0.02016640284431468\n",
            "train loss:0.01884198381991956\n",
            "train loss:0.013129722745257144\n",
            "train loss:0.02283888350989522\n",
            "train loss:0.0065784020561561515\n",
            "train loss:0.06562912041772846\n",
            "train loss:0.02346056188263539\n",
            "train loss:0.04407418577986083\n",
            "train loss:0.0207019587927997\n",
            "=== epoch:20, train acc:0.993, test acc:0.955 ===\n",
            "train loss:0.020095591573753666\n",
            "train loss:0.020778360389520648\n",
            "train loss:0.022835516644376774\n",
            "train loss:0.03061413532907109\n",
            "train loss:0.07883669426638686\n",
            "train loss:0.011182651768732801\n",
            "train loss:0.008848886375927893\n",
            "train loss:0.010940529727139497\n",
            "train loss:0.02454415179755646\n",
            "train loss:0.016044152639041607\n",
            "train loss:0.014241776366104495\n",
            "train loss:0.00918231426939182\n",
            "train loss:0.044256379904447404\n",
            "train loss:0.04051098292444939\n",
            "train loss:0.015462012734000118\n",
            "train loss:0.006874945102927934\n",
            "train loss:0.074367495520882\n",
            "train loss:0.012883493413871032\n",
            "train loss:0.04286778883776945\n",
            "train loss:0.028177134468415358\n",
            "train loss:0.012967749416982272\n",
            "train loss:0.009593348759529177\n",
            "train loss:0.0275032199181385\n",
            "train loss:0.017308220218986217\n",
            "train loss:0.016862890759587947\n",
            "train loss:0.0054684067500113\n",
            "train loss:0.0058412530700159915\n",
            "train loss:0.01570665784352701\n",
            "train loss:0.027615783410330726\n",
            "train loss:0.04935011338542906\n",
            "train loss:0.01774788707453269\n",
            "train loss:0.04186782050576378\n",
            "train loss:0.0326937140914674\n",
            "train loss:0.04715664835049969\n",
            "train loss:0.02107795703067258\n",
            "train loss:0.04291411439841813\n",
            "train loss:0.02869118262601398\n",
            "train loss:0.028687533998259233\n",
            "train loss:0.009974544487347359\n",
            "train loss:0.0102199808408614\n",
            "train loss:0.06618308237547138\n",
            "train loss:0.021443240216879014\n",
            "train loss:0.030031345145718785\n",
            "train loss:0.03861433810138722\n",
            "train loss:0.09098423294501551\n",
            "train loss:0.016253593704368886\n",
            "train loss:0.024561942033564977\n",
            "train loss:0.06112681409169835\n",
            "train loss:0.028390878996715753\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.96\n",
            "Saved Network Parameters!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "시험 데이터에 대한 정확도가 아주 높게 나오는 것을 볼 수 잇다."
      ],
      "metadata": {
        "id": "gFAIpMrd3P1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1번째 층의 가중치 시각화하기**"
      ],
      "metadata": {
        "id": "6u3nX42y6AdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_show(filters, nx=8, margin=3, scale=10):\n",
        "    \"\"\"\n",
        "    c.f. https://gist.github.com/aidiary/07d530d5e08011832b12#file-draw_weight-py\n",
        "    \"\"\"\n",
        "    FN, C, FH, FW = filters.shape\n",
        "    ny = int(np.ceil(FN / nx))\n",
        "\n",
        "    fig = plt.figure()\n",
        "    fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "    for i in range(FN):\n",
        "        ax = fig.add_subplot(ny, nx, i+1, xticks=[], yticks=[])\n",
        "        ax.imshow(filters[i, 0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "        \n",
        "    plt.show()\n",
        "\n",
        "network = SimpleConvNet()\n",
        "# 무작위(랜덤) 초기화 후의 가중치\n",
        "filter_show(network.params['W1'])\n",
        "\n",
        "# 학습된 가중치\n",
        "network.load_params(\"params.pkl\")\n",
        "filter_show(network.params['W1'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "jufu8iN-7W--",
        "outputId": "732229dc-d1c6-4832-9138-77eb975b2466"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 30 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "첫 번째에서는 학습 전 필터는 무작위로 초기화되고 있어 흑백의 정도에 규칙성이 없지만, 두 번째의 학습을 마친 필터는 규칙성 있는 이미지가 되었다.\n",
        "\n",
        "규칙성 있는 필터가 보고 있는 것은 에지(섹상이 바뀐 경계선)와 블롭(국소적으로 덩어리진 영역)이다.\n",
        "\n",
        "이처럼 합성곱 계층의 필터는 에지나 블롭 등의 원시적인 정보를 추출할 수 있다.\n",
        "\n",
        "이런 원시적인 정보가 뒷단 계층에 전달된다는 것이 앞에서 구현한 CNN에서 일어나는 일이다."
      ],
      "metadata": {
        "id": "jtnQPKLY7ugd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **층 깊이에 따른 추출 정보 변화**"
      ],
      "metadata": {
        "id": "NPZflCqc8Gqj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "합성곱 계층을 여러 겹 쌓으면, 층이 깊어지면서 더 복잡하고 추상화된 정보가 추출된다.\n",
        "\n",
        "처음 층은 단순한 에지에 반응하고, 이어서 텍스처에 반응하고, 더 복잡한 사물의 일부에 반응하도록 변화한다.\n",
        "\n",
        "즉, 층이 깊어지면서 뉴런이 반응하는 대상이 단순한 모양에서 '고급'정보로 변화해간다. 다시 말해 사물의 '의미'를 이해하도록 변화한다."
      ],
      "metadata": {
        "id": "Au4SZDzu8U7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **대표적인 CNN - LeNet**"
      ],
      "metadata": {
        "id": "hjiPw81d8i9e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LeNet**은 손글씨 숫자를 인식하는 네트워크이다. 합성곱 계층과 풀링 계층(정확히는 단순히 '원소를 줄이기만'하는 서브샘플링 계층)을 반복하고, 마지막으로 완전연결 계층을 거치면서 결과를 출력한다.\n",
        "\n",
        "LeNet과 '현재의 CNN'의 차이점\n",
        "* LeNet은 활성화 함수로 시그모이드 함수를 사용하지만 현재는 주로 ReLU를 사용한다.\n",
        "* 원래의 LeNet은 서브샘플링을 하여 중간 데이터의 크기를 줄이지만 현재는 최대 풀링이 주류이다."
      ],
      "metadata": {
        "id": "3PIxJ-xt-UzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **대표적인 CNN - AlexNet**"
      ],
      "metadata": {
        "id": "r9oo6kdp-3Ev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LeNet과 비교해 훨씬 최근에 발표된 **AlexNet**은 딥러닝 열풍을 일으키는데 큰 역할을 하였다.\n",
        "\n",
        "구성은 기본적으로 LeNet과 크게 다르지 않지만 다음과 같은 변화가 있다.\n",
        "* 활성화 함수로 ReLU를 이용한다.\n",
        "* LRN(Local Response Normalization)이라는 국소적 정규화를 실시하는 계층을 이용한다.\n",
        "* 드롭아웃을 사용한다.\n",
        "\n",
        "LeNet과 AlexNet에 큰 차이는 없지만, 컴퓨팅 기술의 발전,\n",
        "병렬 계산에 특화된 GPU와 빅데이터, 이것이 딥러닝 발전의 큰 원동력이다."
      ],
      "metadata": {
        "id": "OM467-OU-6Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정리**\n",
        "\n",
        "* **CNN은 지금까지의 완전연결 계층 네트워크에 합성곱 계층과 풀링 계층을 새로 추가한다.**\n",
        "* **합성곱 계층과 풀링 계층은 im2col(이미지를 행렬로 전개하는 함수)을 이용하면 간단하고 효율적으로 구현할 수 있다.**\n",
        "* **CNN을 시각화해보면 계층이 깊어질수록 고급 정보가 추출되는 모습을 확인할 수 있다.**\n",
        "* **대표적인 CNN에는 LeNet과 AlexNet이 있다.**\n",
        "* **딥러닝의 발전에는 빅데이터와 GPU가 크게 기여했다.**"
      ],
      "metadata": {
        "id": "LOWvM1BU_kyi"
      }
    }
  ]
}