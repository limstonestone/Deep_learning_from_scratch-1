{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap05_Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOLHya1xSFvOIp9XEm+nEIl"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **오차역전파법**"
      ],
      "metadata": {
        "id": "-xoPteBEvMWz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "편미분 대신 Backpropagation을 사용할때의 장점?\n",
        "\n",
        "가중치 매개변수의 기울기를 구할때 정직하게 수치 미분을 하는것이 아니고,\n",
        "\n",
        "노드별 미리 정해진 미분을 상수와 같이 사용하므로 계산에 있어서 매우 효율적이다."
      ],
      "metadata": {
        "id": "kwk-zxqovOyr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **곱셈 계층 구현하기**"
      ],
      "metadata": {
        "id": "VjtQqWKQhjQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "MChIc_0zuJX2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MulLayer: # 계산그래프의 곱셈 노드\n",
        "    def __init__(self):\n",
        "        self.x = None\n",
        "        self.y = None\n",
        "\n",
        "    def forward(self, x, y): # 순전파\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        out = x * y\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout): # 역전파\n",
        "        dx = dout * self.y # x와 y를 바꾼다.\n",
        "        dy = dout * self.x\n",
        "\n",
        "        return dx, dy"
      ],
      "metadata": {
        "id": "R5YDGgizocqQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사과 쇼핑 예시에서 순전파 구현\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MulLayer() \n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)\n",
        "price = mul_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(price)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw2xWkCao0oM",
        "outputId": "9170945c-c855-4c66-8540-647e4e3fc50f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "220.00000000000003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 역전파\n",
        "dprice = 1\n",
        "dapple_price, dtax = mul_tax_layer.backward(dprice)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj4uzYUypyBR",
        "outputId": "aa1af8b8-dbb8-440b-a35c-622bf6b3f267"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.2 110.00000000000001 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **덧셈 계층 구현하기**"
      ],
      "metadata": {
        "id": "bDlB5U4KqG7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AddLayer: # 계산 그래프의 덧셈 노드\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        out = x + y\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * 1 # 덧셈 노드는 상류의 값을 여과 없이 하류로 내보낸다.\n",
        "        dy = dout * 1\n",
        "        return dx, dy"
      ],
      "metadata": {
        "id": "WeC0uboorBwG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 사과/귤 쇼핑 예시에서 계산 그래프 구현\n",
        "\n",
        "apple = 100\n",
        "apple_num = 2\n",
        "orange = 150\n",
        "orange_num = 3\n",
        "tax = 1.1\n",
        "\n",
        "# 계층들\n",
        "mul_apple_layer = MulLayer()\n",
        "mul_orange_layer = MulLayer()\n",
        "add_apple_orange_layer = AddLayer()\n",
        "mul_tax_layer = MulLayer()\n",
        "\n",
        "# 순전파\n",
        "apple_price = mul_apple_layer.forward(apple, apple_num)  # (1)\n",
        "orange_price = mul_orange_layer.forward(orange, orange_num)  # (2)\n",
        "all_price = add_apple_orange_layer.forward(apple_price, orange_price)  # (3)\n",
        "price = mul_tax_layer.forward(all_price, tax)  # (4)\n",
        "\n",
        "# 역전파\n",
        "dprice = 1\n",
        "dall_price, dtax = mul_tax_layer.backward(dprice)  # (4)\n",
        "dapple_price, dorange_price = add_apple_orange_layer.backward(dall_price)  # (3)\n",
        "dorange, dorange_num = mul_orange_layer.backward(dorange_price)  # (2)\n",
        "dapple, dapple_num = mul_apple_layer.backward(dapple_price)  # (1)\n",
        "\n",
        "print(price)\n",
        "print(dapple_num, dapple, dorange, dorange_num, dtax)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw4IGG7FrMRY",
        "outputId": "d3b2fd1b-cc94-445e-d276-6e7201d445c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "715.0000000000001\n",
            "110.00000000000001 2.2 3.3000000000000003 165.0 650\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ReLU 계층 구현하기**"
      ],
      "metadata": {
        "id": "ZUHe7x1lsLN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "    \n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0) # 활성화 함수 Relu의 수식\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "RI-lLKQzta2Y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([[1.0, -0.5], [-2.0, 3.0]])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J3dEnGHRt8uZ",
        "outputId": "c5f20fdd-7ffc-4eaf-ea31-2b0a1f590247"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  -0.5]\n",
            " [-2.   3. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = (x <= 0)\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "av2IXIynuHjw",
        "outputId": "0a63f812-a354-497e-fa63-51a54dfed045"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[False  True]\n",
            " [ True False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sigmoid 계층 구현하기**"
      ],
      "metadata": {
        "id": "Hq5MTMRBuROa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = 1 / (1 + np.exp(-x))\n",
        "        self.out = out\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out # Sigmoid의 정해진 미분 dout * y(1-y)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "YjVnwm1VuXBb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Affine 계층 구현하기**"
      ],
      "metadata": {
        "id": "U23-zm4SwVBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "신경망의 순전파때 수행하는 행렬의 곱을 기하학에서 **어파인 변환(affine transform)**이라고 한다.  예) $ Y = X \\cdot\\ W + B$"
      ],
      "metadata": {
        "id": "pl5xIHFvwclX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.x = None\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        out = np.dot(x, self.W) + self. b # affine transform\n",
        "        \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = np.dot(dout, self.W.T) # 행렬의 역전파\n",
        "        self.dW = np.dot(self.x.T, dout)\n",
        "        self.db = np.sum(dout, axis=0)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "5O6K1hK3wrYi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Softmax-with-Loss 계층 구현하기**"
      ],
      "metadata": {
        "id": "_D6OKbSLzex3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "신경망의 출력(Softmax의 출력)이 정답 레이블과 가까워지도록 가중치 매개변수의 값을 조정하는 것이 신경망 학습의 목적이다.\n",
        "\n",
        "따라서 신경망의 출력과 정답 레이블의 오차를 효율적으로 앞 계층에 전달 해야한다. -> 역전파"
      ],
      "metadata": {
        "id": "HIXqW-xu2iFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None # 손실\n",
        "        self.y = None # softmax의 출력\n",
        "        self.t = None # 정답 레이블(원-핫 벡터)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        dx = (self.y - self.t) / batch_size # 오차를 앞 계층에 전달한다. (y - t)\n",
        "\n",
        "        return dx"
      ],
      "metadata": {
        "id": "OtGVOePH0tAA"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **신경망 학습의 전체 그림**"
      ],
      "metadata": {
        "id": "kaFrp17S3L_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**전제**\n",
        "    \n",
        "　신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다. 신경망 학습은 다음과 같이 4단계로 수행한다.\n",
        "\n",
        "**1단계 - 미니배치**\n",
        "\n",
        "　훈련 데이터 중 일부를 무작위로 가져온다.. 이렇게 선별한 데이터를 미니배치라 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표이다.\n",
        "\n",
        "**2단계 - 기울기 산출**\n",
        "\n",
        "　미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
        "\n",
        "**3단계 - 매개변수 갱신**\n",
        "\n",
        "　가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
        "\n",
        "**4단계 - 반복**\n",
        "\n",
        "　1~3단계를 반복한다."
      ],
      "metadata": {
        "id": "3D09Ouos4eC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **오차역전파법을 적용한 신경망 구현하기**"
      ],
      "metadata": {
        "id": "FEgri7C54vf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "my_path = '/content/drive/MyDrive/밑바닥부터시작하는딥러닝/deep-learning-from-scratch-master'\n",
        "\n",
        "import sys\n",
        "sys.path.append(my_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ammyIfMc5lwO",
        "outputId": "dad34790-383b-4536-e3d0-8d5c8471e63b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from common.layers import *\n",
        "from common.gradient import numerical_gradient\n",
        "from collections import OrderedDict\n",
        "\n",
        "\n",
        "class TwoLayerNet:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "        self.params['b1'] = np.zeros(hidden_size)\n",
        "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
        "        self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict() # 신경망의 계층을 '순서가 있는' 딕셔너리에 보관한다. 즉 딕셔너리에 추가한 순서를 기억한다.\n",
        "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
        "\n",
        "        self.lastLayer = SoftmaxWithLoss()\n",
        "        \n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def loss(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        return self.lastLayer.forward(y, t)\n",
        "    \n",
        "    def accuracy(self, x, t):\n",
        "        y = self.predict(x)\n",
        "        y = np.argmax(y, axis=1)\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
        "        \n",
        "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "        return accuracy\n",
        "        \n",
        "    # x : 입력 데이터, t : 정답 레이블\n",
        "    def numerical_gradient(self, x, t):\n",
        "        loss_W = lambda W: self.loss(x, t)\n",
        "        \n",
        "        grads = {}\n",
        "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "        \n",
        "        return grads\n",
        "        \n",
        "    def gradient(self, x, t):\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.lastLayer.backward(dout)\n",
        "        \n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads"
      ],
      "metadata": {
        "id": "1WmE3zSP5Gpn"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **오차역전파법으로 구한 기울기 검증하기**"
      ],
      "metadata": {
        "id": "rdjvXeNB5TaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "오차역전파법이 완벽해 보이지만, 사실은 구현하기가 복잡하기 때문에 종종 실수가 생기기도 한다.\n",
        "\n",
        "따라서 연산이 느리지만 구현이 쉬운 수치미분을 통한 결과값과 오차역전파법을 통한 결과값을 비교하여 오차역전파법을 정확히 구현했는지 확인한다.\n",
        "\n",
        "이를 **기울기 확인(gradient check)**이라고 한다."
      ],
      "metadata": {
        "id": "UiDnV0J96miD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.mnist import load_mnist\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "x_batch = x_train[:3]\n",
        "t_batch = t_train[:3]\n",
        "\n",
        "grad_numerical = network.numerical_gradient(x_batch, t_batch)\n",
        "grad_backprop = network.gradient(x_batch, t_batch)\n",
        "\n",
        "# 각 가중치의 절대 오차의 평균을 구한다.\n",
        "for key in grad_numerical.keys():\n",
        "    diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n",
        "    print(key + \":\" + str(diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uaxga2eO7vhe",
        "outputId": "f2ceac71-f406-44c6-d5fb-605a91e393b5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W1:3.5092118536502753e-10\n",
            "b1:1.9991539724481057e-09\n",
            "W2:5.447185221690474e-09\n",
            "b2:1.3973421384844143e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **오차역전파법을 사용한 학습 구현하기**"
      ],
      "metadata": {
        "id": "XiqFYktf7sV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataset.mnist import load_mnist\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "iters_num = 10000\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "    batch_mask = np.random.choice(train_size, batch_size)\n",
        "    x_batch = x_train[batch_mask]\n",
        "    t_batch = t_train[batch_mask]\n",
        "    \n",
        "    # 기울기 계산\n",
        "    #grad = network.numerical_gradient(x_batch, t_batch) # 수치 미분 방식\n",
        "    grad = network.gradient(x_batch, t_batch) # 오차역전파법 방식(훨씬 빠르다)\n",
        "    \n",
        "    # 갱신\n",
        "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "        network.params[key] -= learning_rate * grad[key]\n",
        "    \n",
        "    loss = network.loss(x_batch, t_batch)\n",
        "    train_loss_list.append(loss)\n",
        "    \n",
        "    if i % iter_per_epoch == 0:\n",
        "        train_acc = network.accuracy(x_train, t_train)\n",
        "        test_acc = network.accuracy(x_test, t_test)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        print(train_acc, test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8Ap8mYN7FmN",
        "outputId": "144e9894-5d0c-46a5-8c50-fff5c04e7ba2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.11591666666666667 0.1159\n",
            "0.9044333333333333 0.9072\n",
            "0.9229166666666667 0.9262\n",
            "0.9358666666666666 0.936\n",
            "0.9432833333333334 0.941\n",
            "0.9515166666666667 0.9481\n",
            "0.9558166666666666 0.9525\n",
            "0.9585 0.957\n",
            "0.9626 0.9606\n",
            "0.9669666666666666 0.9648\n",
            "0.9672166666666666 0.9626\n",
            "0.9719 0.9679\n",
            "0.9734 0.9673\n",
            "0.9755833333333334 0.97\n",
            "0.97595 0.9691\n",
            "0.97815 0.972\n",
            "0.97955 0.9713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **정리**\n",
        "\n",
        "* **계산 그래프를 이용하면 계산 과정을 시각적으로 파악할 수 있다.**\n",
        "* **계산 그래프의 노드는 국소적 계산으로 구성된다. 국소적 계산을 조합해 전체 계산을 구성한다.**\n",
        "* **계산 그래프의 순전파는 통상의 계산을 수행한다. 한편, 계산그래프의 역전파로는 각 노드의 미분을 구할 수 있다.**\n",
        "* **신경망의 구성 요소를 계층으로 구현하여 기울기를 효율적으로 계산할 수 있다(오차역전파법).**\n",
        "* **수치 미분과 오차역전파법의 결과를 비교하면 오차역전파법의 구현에 잘못이 없는지 확인할 수 있다(기울기 확인),**"
      ],
      "metadata": {
        "id": "hH3KVG5J7Qru"
      }
    }
  ]
}